#!/usr/bin/env python3
"""
Log Analyzer for RAG Application

This script helps you analyze and view logs generated by the logging service.
"""

import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd

class LogAnalyzer:
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        
    def get_available_dates(self) -> List[str]:
        """Get all available dates with log files."""
        dates = set()
        for log_file in self.log_dir.glob("*.jsonl"):
            # Extract date from filename (e.g., queries_20241201.jsonl -> 20241201)
            parts = log_file.stem.split("_")
            if len(parts) >= 2:
                dates.add(parts[-1])
        return sorted(list(dates))
    
    def load_logs(self, date: str, log_type: str = "all") -> List[Dict[str, Any]]:
        """Load logs for a specific date and type."""
        logs = []
        
        if log_type == "all":
            log_types = ["queries", "responses", "uploads", "errors"]
        else:
            log_types = [log_type]
            
        for log_type_name in log_types:
            log_file = self.log_dir / f"{log_type_name}_{date}.jsonl"
            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            logs.append(json.loads(line.strip()))
                            
        return sorted(logs, key=lambda x: x.get('timestamp', ''))
    
    def analyze_queries(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze query patterns and statistics."""
        query_logs = [log for log in logs if log.get('event_type') == 'user_query']
        
        if not query_logs:
            return {"message": "No query logs found"}
        
        analysis = {
            "total_queries": len(query_logs),
            "unique_users": len(set(log.get('user_id') for log in query_logs if log.get('user_id'))),
            "average_query_length": sum(len(log.get('query', '')) for log in query_logs) / len(query_logs),
            "most_active_user": None,
            "query_frequency": {}
        }
        
        # Find most active user
        user_counts = {}
        for log in query_logs:
            user_id = log.get('user_id')
            if user_id:
                user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        if user_counts:
            analysis["most_active_user"] = max(user_counts, key=user_counts.get)
        
        # Query frequency analysis
        for log in query_logs:
            query = log.get('query', '').lower()
            # Extract key words (simple approach)
            words = query.split()[:3]  # First 3 words
            key = ' '.join(words)
            analysis["query_frequency"][key] = analysis["query_frequency"].get(key, 0) + 1
        
        return analysis
    
    def analyze_responses(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze response patterns and performance."""
        response_logs = [log for log in logs if log.get('event_type') == 'rag_response']
        
        if not response_logs:
            return {"message": "No response logs found"}
        
        processing_times = [log.get('processing_time_seconds', 0) for log in response_logs]
        
        analysis = {
            "total_responses": len(response_logs),
            "average_processing_time": sum(processing_times) / len(processing_times) if processing_times else 0,
            "max_processing_time": max(processing_times) if processing_times else 0,
            "min_processing_time": min(processing_times) if processing_times else 0,
            "average_response_length": sum(len(log.get('response', '')) for log in response_logs) / len(response_logs),
            "slow_queries": []
        }
        
        # Find slow queries (>5 seconds)
        slow_threshold = 5.0
        for log in response_logs:
            if log.get('processing_time_seconds', 0) > slow_threshold:
                analysis["slow_queries"].append({
                    "query": log.get('query', ''),
                    "processing_time": log.get('processing_time_seconds', 0),
                    "timestamp": log.get('timestamp', '')
                })
        
        return analysis
    
    def analyze_errors(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze error patterns."""
        error_logs = [log for log in logs if log.get('event_type') == 'error']
        
        if not error_logs:
            return {"message": "No error logs found"}
        
        error_types = {}
        for log in error_logs:
            error_type = log.get('error_type', 'unknown')
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        analysis = {
            "total_errors": len(error_logs),
            "error_types": error_types,
            "most_common_error": max(error_types, key=error_types.get) if error_types else None,
            "recent_errors": error_logs[-10:]  # Last 10 errors
        }
        
        return analysis
    
    def generate_report(self, date: str = None) -> Dict[str, Any]:
        """Generate a comprehensive report for a specific date."""
        if not date:
            date = datetime.now().strftime('%Y%m%d')
        
        logs = self.load_logs(date)
        
        report = {
            "date": date,
            "total_logs": len(logs),
            "queries": self.analyze_queries(logs),
            "responses": self.analyze_responses(logs),
            "errors": self.analyze_errors(logs),
            "uploads": len([log for log in logs if log.get('event_type') == 'file_upload'])
        }
        
        return report
    
    def print_report(self, report: Dict[str, Any]):
        """Print a formatted report."""
        print(f"\n{'='*60}")
        print(f"LOG ANALYSIS REPORT - {report['date']}")
        print(f"{'='*60}")
        
        print(f"\nüìä OVERVIEW:")
        print(f"   Total Logs: {report['total_logs']}")
        print(f"   File Uploads: {report['uploads']}")
        
        # Queries
        queries = report['queries']
        if 'message' not in queries:
            print(f"\nüîç QUERIES:")
            print(f"   Total Queries: {queries['total_queries']}")
            print(f"   Unique Users: {queries['unique_users']}")
            print(f"   Average Query Length: {queries['average_query_length']:.1f} characters")
            if queries['most_active_user']:
                print(f"   Most Active User: {queries['most_active_user']}")
        
        # Responses
        responses = report['responses']
        if 'message' not in responses:
            print(f"\nü§ñ RESPONSES:")
            print(f"   Total Responses: {responses['total_responses']}")
            print(f"   Average Processing Time: {responses['average_processing_time']:.2f} seconds")
            print(f"   Max Processing Time: {responses['max_processing_time']:.2f} seconds")
            print(f"   Average Response Length: {responses['average_response_length']:.1f} characters")
            if responses['slow_queries']:
                print(f"   Slow Queries (>5s): {len(responses['slow_queries'])}")
        
        # Errors
        errors = report['errors']
        if 'message' not in errors:
            print(f"\n‚ùå ERRORS:")
            print(f"   Total Errors: {errors['total_errors']}")
            if errors['error_types']:
                print(f"   Error Types: {dict(errors['error_types'])}")
            if errors['most_common_error']:
                print(f"   Most Common Error: {errors['most_common_error']}")
        
        print(f"\n{'='*60}")

def main():
    parser = argparse.ArgumentParser(description="Analyze RAG application logs")
    parser.add_argument("--date", help="Date to analyze (YYYYMMDD format, defaults to today)")
    parser.add_argument("--list-dates", action="store_true", help="List available dates")
    parser.add_argument("--type", choices=["queries", "responses", "uploads", "errors", "all"], 
                       default="all", help="Type of logs to analyze")
    parser.add_argument("--raw", action="store_true", help="Show raw log data")
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer()
    
    if args.list_dates:
        dates = analyzer.get_available_dates()
        print("Available dates with logs:")
        for date in dates:
            print(f"  {date}")
        return
    
    if not args.date:
        args.date = datetime.now().strftime('%Y%m%d')
    
    if args.raw:
        logs = analyzer.load_logs(args.date, args.type)
        print(json.dumps(logs, indent=2))
    else:
        report = analyzer.generate_report(args.date)
        analyzer.print_report(report)

if __name__ == "__main__":
    main()
